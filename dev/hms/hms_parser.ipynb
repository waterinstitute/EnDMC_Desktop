{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project: Amite HMS',\n",
       " '     Description: Includes main models from TWIF Amite Study Jan112021',\n",
       " '     Version: 4.10',\n",
       " '     Filepath Separator: \\\\',\n",
       " '     DSS File Name: Amite_HMS.dss',\n",
       " '     Time Zone ID: America/Chicago',\n",
       " 'End:',\n",
       " '',\n",
       " 'Precipitation: Specified Hyetographs',\n",
       " '     Filename: Specified_Hyetographs.met',\n",
       " '     Description: Processed from Stage IV Gridded Data',\n",
       " '     Last Modified Date: 13 January 2023',\n",
       " '     Last Modified Time: 16:49',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Aug2016 Cali wET',\n",
       " '     Filename: Aug2016_Cali_wET.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2023',\n",
       " '     Last Modified Time: 17:24',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Isaac Cali wET -QAQC',\n",
       " '     Filename: Isaac_Cali_wET__QAQC.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 17:34',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Barry Cali wET',\n",
       " '     Filename: Barry_Cali_wET.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 17:35',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Aug2016 Val (Gustav)',\n",
       " '     Filename: Aug2016_Val__Gustav_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 18:11',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Barry Vali (Gustav)',\n",
       " '     Filename: Barry_Vali__Gustav_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 18:12',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Isaac Vali (Gustav)',\n",
       " '     Filename: Isaac_Vali__Gustav_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 18:17',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Aug2016 Vali (Katrina)',\n",
       " '     Filename: Aug2016_Vali__Katrina_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 25 January 2021',\n",
       " '     Last Modified Time: 21:24',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Barry Vali (Katrina)',\n",
       " '     Filename: Barry_Vali__Katrina_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 25 January 2021',\n",
       " '     Last Modified Time: 21:24',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Isaac Vali (Katrina)',\n",
       " '     Filename: Isaac_Vali__Katrina_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 18:27',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Consensus (Generic)',\n",
       " '     Filename: Consensus__Generic_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 15 September 2022',\n",
       " '     Last Modified Time: 17:22',\n",
       " 'End:',\n",
       " '',\n",
       " 'Basin: Consensus (March 2016)',\n",
       " '     Filename: Consensus__March_2016_.basin',\n",
       " '     Description: ',\n",
       " '     Last Modified Date: 12 January 2021',\n",
       " '     Last Modified Time: 19:41',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: Aug2016',\n",
       " '     FileName: Aug2016.control',\n",
       " '     Description: Flood of August 2016',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: Barry',\n",
       " '     FileName: Barry.control',\n",
       " '     Description: Hurricane Barry',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: Gustav',\n",
       " '     FileName: Gustav.control',\n",
       " '     Description: Hurricanes Gustav and Ike',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: Isaac',\n",
       " '     FileName: Isaac.control',\n",
       " '     Description: Hurricane Isaac',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: Katrina',\n",
       " '     FileName: Katrina.control',\n",
       " '     Description: Hurricane Katrina',\n",
       " 'End:',\n",
       " '',\n",
       " 'Control: March 2016',\n",
       " '     FileName: March_2016.control',\n",
       " '     Description: March 2016',\n",
       " 'End:',\n",
       " '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "prj = \"Z:/Amite/Amite_LWI/Models/Amite_HMS/Amite_HMS.hms\"\n",
    "prj_dir, prj_file_tail = os.path.split(prj)\n",
    "prj_name = prj_file_tail.split(\".\")[0]\n",
    "\n",
    "with open(\"Z:/Amite/Amite_LWI/Models/Amite_HMS/Amite_HMS.hms\", \"r\") as f:\n",
    "     lines = f.readlines()\n",
    "lines = [s.strip('\\n') for s in lines]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "keyValueList = copy.deepcopy(lines)  \n",
    "\n",
    "nest_start = 0\n",
    "nestList = []\n",
    "for i,v in enumerate(lines):\n",
    "        if v == 'End:':\n",
    "                # If not the beginning of the file, skip a blank line (+1) for the start of the subList.\n",
    "                if len(nestList) > 0:\n",
    "                        nestList.append(lines[nest_start+1:i])\n",
    "                else:\n",
    "                        nestList.append(lines[nest_start:i])\n",
    "                nest_start = i+1\n",
    "# nestList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Control', 'Project', 'Basin', 'Precipitation']"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique headers [Project, Control, Basin, etc.] as a list.\n",
    "headers = []\n",
    "for subList in nestList:\n",
    "    headers.append(subList[0].split(\":\")[0])\n",
    "unique_headers = list(set(headers))\n",
    "unique_headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Control Files', 'Basin', 'Precipitaion', 'Project'])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary based on keys using the unique headers with values of Title, Filename, and Description\n",
    "kv = {}\n",
    "kv['Control Files'] = {}\n",
    "kv['Basin'] = {}\n",
    "kv['Precipitaion'] = {}\n",
    "headers_with_same_parsing = ['Control', 'Basin', 'Precipitation']\n",
    "\n",
    "for subList in nestList:\n",
    "    header = subList[0].split(\":\")[0]\n",
    "    title = subList[0].split(\":\")[1]\n",
    "    find_str = 'Description'\n",
    "    description = [s for s in subList if find_str in s][0].split(\":\")[1:][0].strip()\n",
    "\n",
    "    if header == 'Project':\n",
    "        find_str = 'File Name'\n",
    "        filename = [s for s in subList if find_str in s][0].split(\":\")[1:][0].strip()\n",
    "        kv['Project'] = {\n",
    "            'Title': title,\n",
    "            'Project Output DSS File': filename,\n",
    "            'Description': description\n",
    "        }\n",
    "    \n",
    "    if any(header == i for i in headers_with_same_parsing):\n",
    "        find_str = 'filename'\n",
    "        filename = [s for s in subList if find_str in s.lower()][0].split(\":\")[1:][0].strip()\n",
    "        kv['Project'][title] = {\n",
    "            'File Name': filename,\n",
    "            'Description': description\n",
    "        }\n",
    "\n",
    "kv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.access',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.dss',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.gage',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.hms',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.log',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.out',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.pdata',\n",
       " 'Z:/Amite/Amite_LWI/Models/Amite_HMS\\\\Amite_HMS.run']"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os\n",
    "# prj_name = kv['Project']['Title']\n",
    "# Get project files\n",
    "prj_files_List = []\n",
    "\n",
    "for pFile in glob.glob(rf'{prj_dir}/{prj_name}.*'):\n",
    "        prj_files_List.append(pFile)\n",
    "\n",
    "prj_files_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\Amite_HMS_TimeSeries.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'Amite_HMS_TimeSeries'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\rainfall_stageIV.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'rainfall_stageIV'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\Specified_Hyetographs.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'Specified_Hyetographs'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\USGS_observed.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'USGS_observed'}]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add application_date from prj_file modified date\n",
    "modTimeUnix = os.path.getmtime(prj) \n",
    "kv['application_date'] = datetime.fromtimestamp(modTimeUnix).strftime('%Y-%m-%d')\n",
    "\n",
    "# if args.dss, get dss input files\n",
    "input_dss_dir = r\"Z:\\Amite\\Amite_LWI\\Models\\Amite_HMS\\data\"\n",
    "dss_files_list = []\n",
    "for pFile in glob.glob(rf'{input_dss_dir}/*.dss'):\n",
    "        dss_files_list.append(pFile)\n",
    "\n",
    "# dss_files_list\n",
    "dss_common_files_input = []\n",
    "if len(dss_files_list)>0:\n",
    "        for f in dss_files_list:\n",
    "                head, tail = os.path.split(f)\n",
    "                dss_title = tail.split(\".\")[0]\n",
    "                dss_common_files_input.append(\n",
    "                        {\n",
    "                                \"description\": \"User Added from Input DSS File Directory\",\n",
    "                                \"location\": f,\n",
    "                                \"source_dataset\": None,\n",
    "                                \"title\": dss_title\n",
    "                        },\n",
    "                )\n",
    "dss_common_files_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'description': 'The HMS Project File',\n",
       "   'location': 'Amite_HMS.hms',\n",
       "   'source_dataset': None,\n",
       "   'title': 'Project File'},\n",
       "  {'description': 'There may be multiple basins in the HMS model project',\n",
       "   'location': 'Z:/Amite/Amite_LWI/Models/Amite_HMS/*.basin',\n",
       "   'source_dataset': None,\n",
       "   'title': 'Basin Files'},\n",
       "  {'description': 'There may be multiple Meteorological Models',\n",
       "   'location': 'Z:/Amite/Amite_LWI/Models/Amite_HMS/*.met',\n",
       "   'source_dataset': None,\n",
       "   'title': 'Meteorological Model Files'},\n",
       "  {'description': 'There may be multiple control specifications.',\n",
       "   'location': 'Z:/Amite/Amite_LWI/Models/Amite_HMS/*.control',\n",
       "   'source_dataset': None,\n",
       "   'title': 'Control Specification Files'}],\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\Amite_HMS_TimeSeries.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'Amite_HMS_TimeSeries'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\rainfall_stageIV.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'rainfall_stageIV'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\Specified_Hyetographs.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'Specified_Hyetographs'},\n",
       " {'description': 'User Added from Input DSS File Directory',\n",
       "  'location': 'Z:\\\\Amite\\\\Amite_LWI\\\\Models\\\\Amite_HMS\\\\data\\\\USGS_observed.dss',\n",
       "  'source_dataset': None,\n",
       "  'title': 'USGS_observed'}]"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the model application Json template, del unnecessary keys, update, add, export \n",
    "with open(r\"C:\\py\\hec_meta_extract\\example\\input\\json\\hms_model_application.json\", 'r') as f:\n",
    "            model_template_json = json.load(f)\n",
    "\n",
    "# keys to drop from json template\n",
    "drop_keys = ['_id', 'linked_resources', 'common_parameters', 'common_software_version', 'authors', \n",
    "'spatial_extent_resolved', 'spatial_valid_extent_resolved', 'temporal_extent', 'temporal_resolution', \n",
    "'spatial_valid_extent', 'common_files_details', 'grid']\n",
    "for key in drop_keys:\n",
    "    del model_template_json[key]\n",
    "\n",
    "# set basic keywords\n",
    "model_template_json['keywords'] = ['hec-hms','hec','hms','hydrology','model','lwi']\n",
    "\n",
    "model_template_json['purpose'] = kv['Project']['Description']\n",
    "model_template_json['description'] = kv['Project']['Description']\n",
    "model_template_json['title'] = f\"{kv['Project']['Title']} HEC-HMS Model\"\n",
    "\n",
    "# common_files_details[]\n",
    "model_template_json['common_files_details'] = []\n",
    "model_template_json['common_files_details'].append(\n",
    "    [{\n",
    "        \"description\": \"The HMS Project File\",\n",
    "        \"location\": prj_file_tail,\n",
    "        \"source_dataset\": None,\n",
    "        \"title\": \"Project File\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"There may be multiple basins in the HMS model project\",\n",
    "        \"location\": f\"{prj_dir}/*.basin\",\n",
    "        \"source_dataset\": None,\n",
    "        \"title\": \"Basin Files\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"There may be multiple Meteorological Models\",\n",
    "        \"location\": f\"{prj_dir}/*.met\",\n",
    "        \"source_dataset\": None,\n",
    "        \"title\": \"Meteorological Model Files\"\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"There may be multiple control specifications.\",\n",
    "        \"location\": f\"{prj_dir}/*.control\",\n",
    "        \"source_dataset\": None,\n",
    "        \"title\": \"Control Specification Files\"\n",
    "    }]\n",
    ")\n",
    "model_template_json['common_files_details'].extend(dss_common_files_input)\n",
    "model_template_json['common_files_details']\n",
    "# Map kv dictionary to the model application json\n",
    "# application_date, spatial_extent[0], common_files_details[..], purpose, description, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output model application json\n",
    "output_prj_json = f'{prj_name}_model_application.json'\n",
    "with open(output_prj_json, \"w\") as outfile:\n",
    "    json.dump(model_template_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the simulation Json template, del unnecessary keys, update, add, export\n",
    "\n",
    "with open(r\"C:\\py\\hec_meta_extract\\example\\input\\json\\hms_simulation.json\", 'r') as f:\n",
    "            sim_json = json.load(f)\n",
    "# model_template_json\n",
    "# keys to drop from json template\n",
    "drop_keys = ['_id', 'model_application', 'linked_resources', 'input_files', 'output_files', 'type']\n",
    "for key in drop_keys:\n",
    "    del sim_json[key]\n",
    "\n",
    "\n",
    "\n",
    "# set basic keywords\n",
    "sim_json['keywords'] = ['hec-hms','hec','hms','hydrology','simulation','lwi']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Run: August 2016',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: August_2016.log',\n",
       "  '     DSS File: August_2016.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 31 January 2023',\n",
       "  '     Last Execution Time: 20:11:32',\n",
       "  '     Basin: Consensus (Generic)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: Aug2016',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:'],\n",
       " ['Run: Barry',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: Barry.log',\n",
       "  '     DSS File: Barry.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 25 January 2021',\n",
       "  '     Last Execution Time: 19:17:16',\n",
       "  '     Basin: Consensus (Generic)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: Barry',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:'],\n",
       " ['Run: Gustav',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: Gustav.log',\n",
       "  '     DSS File: Gustav.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 25 January 2021',\n",
       "  '     Last Execution Time: 21:05:08',\n",
       "  '     Basin: Consensus (Generic)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: Gustav',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:'],\n",
       " ['Run: Isaac',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: Isaac.log',\n",
       "  '     DSS File: Isaac.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 25 January 2021',\n",
       "  '     Last Execution Time: 19:39:20',\n",
       "  '     Basin: Consensus (Generic)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: Isaac',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:'],\n",
       " ['Run: Katrina',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: Katrina.log',\n",
       "  '     DSS File: Katrina.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 25 January 2021',\n",
       "  '     Last Execution Time: 21:24:19',\n",
       "  '     Basin: Consensus (Generic)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: Katrina',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:'],\n",
       " ['Run: March 2016',\n",
       "  '     Default Description: Yes',\n",
       "  '     Log File: March_2016.log',\n",
       "  '     DSS File: March_2016.dss',\n",
       "  '     Is Save Spatial Results: No',\n",
       "  '     Last Modified Date: 13 January 2023',\n",
       "  '     Last Modified Time: 16:36:46',\n",
       "  '     Last Execution Date: 25 January 2021',\n",
       "  '     Last Execution Time: 21:32:47',\n",
       "  '     Basin: Consensus (March 2016)',\n",
       "  '     Precip: Specified Hyetographs',\n",
       "  '     Control: March 2016',\n",
       "  '     Time-Series Output: Save All',\n",
       "  '     Time Series Results Manager Start:',\n",
       "  '     Time Series Results Manager End:']]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open .run file\n",
    "with open(os.path.join(prj_dir,f'{prj_name}.run'), 'r') as r:\n",
    "    run_file = r.readlines()\n",
    "run_file = [s.strip('\\n') for s in run_file]\n",
    "# run_file\n",
    "\n",
    "line_start = 0\n",
    "runList = []\n",
    "for i,v in enumerate(run_file):\n",
    "        if v == 'End:':\n",
    "                # If not the beginning of the file, skip a blank line (+1) for the start of the subList.\n",
    "                if len(runList) > 0:\n",
    "                        runList.append(run_file[line_start+1:i])\n",
    "                else:\n",
    "                        runList.append(run_file[line_start:i])\n",
    "                line_start = i+1\n",
    "runList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus__Generic_.basin\n",
      "Consensus__Generic_.basin\n",
      "Consensus__Generic_.basin\n",
      "Consensus__Generic_.basin\n",
      "Consensus__Generic_.basin\n",
      "Consensus__March_2016_.basin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'August 2016': {'Basin': 'Consensus (Generic)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'August_2016.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Aug2016',\n",
       "  'Description': 'Flood of August 2016',\n",
       "  'Start Date': '5 August 2016',\n",
       "  'End Date': '17 August 2016',\n",
       "  'Time Interval': '60'},\n",
       " 'Barry': {'Basin': 'Consensus (Generic)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'Barry.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Barry',\n",
       "  'Description': 'Hurricane Barry',\n",
       "  'Start Date': '2 July 2019',\n",
       "  'End Date': '25 July 2019',\n",
       "  'Time Interval': '60'},\n",
       " 'Gustav': {'Basin': 'Consensus (Generic)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'Gustav.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Gustav',\n",
       "  'Description': 'Hurricanes Gustav and Ike',\n",
       "  'Start Date': '21 August 2008',\n",
       "  'End Date': '19 September 2008',\n",
       "  'Time Interval': '60'},\n",
       " 'Isaac': {'Basin': 'Consensus (Generic)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'Isaac.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Isaac',\n",
       "  'Description': 'Hurricane Isaac',\n",
       "  'Start Date': '12 August 2012',\n",
       "  'End Date': '10 September 2012',\n",
       "  'Time Interval': '60'},\n",
       " 'Katrina': {'Basin': 'Consensus (Generic)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'Katrina.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Katrina',\n",
       "  'Description': 'Hurricane Katrina',\n",
       "  'Start Date': '13 August 2005',\n",
       "  'End Date': '8 September 2005',\n",
       "  'Time Interval': '60'},\n",
       " 'March 2016': {'Basin': 'Consensus (March 2016)',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       "  'DSS File': 'March_2016.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'March 2016',\n",
       "  'Description': 'March 2016',\n",
       "  'Start Date': '1 March 2016',\n",
       "  'End Date': '30 March 2016',\n",
       "  'Time Interval': '60'}}"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_kv = {}\n",
    "for subList in runList:\n",
    "    title = subList[0].split(\":\")[1].strip()\n",
    "    # print(title)\n",
    "    sim_kv[title] = {}\n",
    "    findList = ['Basin', 'DSS File', 'Precip', 'Control']\n",
    "    parameters = {}\n",
    "    for find_key in findList:\n",
    "        found_value = [s for s in subList if find_key in s][0].split(\":\")[1:][0].strip()\n",
    "        sim_kv[title][find_key] = found_value\n",
    "        \n",
    "        # Add data from each simulation's control file.\n",
    "        if find_key == 'Control':\n",
    "            control_name = sim_kv[title][find_key].replace(\" \",\"_\").replace(\"(\",\"_\").replace(\")\",\"_\") + '.control'\n",
    "            control_file = os.path.join(prj_dir, control_name)\n",
    "            \n",
    "            with open(control_file, 'r') as c:\n",
    "                c_file = c.readlines()\n",
    "            \n",
    "            c_file  = [s.strip('\\n') for s in c_file]\n",
    "            c_findList = ['Description', 'Start Date', 'End Date', 'Time Interval']\n",
    "            \n",
    "            for c_find_key in c_findList:\n",
    "                found_value = [s for s in c_file if c_find_key in s][0].split(\":\")[1:][0].strip()\n",
    "                sim_kv[title][c_find_key] = found_value\n",
    "        \n",
    "        # Add data from each simulations's basin file\n",
    "        parameterList = []\n",
    "        if find_key == 'Basin':\n",
    "            basin_name = sim_kv[title][find_key].replace(\" \",\"_\").replace(\"(\",\"_\").replace(\")\",\"_\") + '.basin'\n",
    "            basin_file = os.path.join(prj_dir, basin_name)\n",
    "            # print(basin_file)\n",
    "            print(basin_name)\n",
    "\n",
    "            with open(basin_file, 'r') as b:\n",
    "                b_file = b.readlines()\n",
    "            # print (b_file)\n",
    "            b_file  = [s.strip('\\n') for s in b_file]\n",
    "\n",
    "            line_start = 0\n",
    "            basinList = []\n",
    "            # print (b_file)\n",
    "            for i,v in enumerate(b_file):\n",
    "                # print (v + '\\n')\n",
    "                if (v == 'End:'):\n",
    "                    # print(i, v)\n",
    "                    # If not the beginning of the file, skip a blank line (+1) for the start of the subList.\n",
    "                    if len(basinList) > 0:\n",
    "                            basinList.append(b_file[line_start+1:i])\n",
    "                    else:\n",
    "                            basinList.append(b_file[line_start:i])\n",
    "                    line_start = i+1\n",
    "\n",
    "\n",
    "            # List of Parameters to look for in each .basin file. Each Parameter will be added as a key to a temporary dictionary before formatting.\n",
    "            b_findList = ['Canopy', 'LossRate', 'Transform', 'Baseflow', 'Route']\n",
    "            params = {}\n",
    "            # initialize empty lists for each parameter key\n",
    "            for key in b_findList:\n",
    "                    params[key] = []\n",
    "            # For each line of each element block in a .basin file, look for each parameter (key) in b_findList \n",
    "            for el in basinList:\n",
    "                for line in el:\n",
    "                    for key in b_findList:\n",
    "                        \n",
    "                        if f'{key}: ' in line:\n",
    "                            # Append the parameter values to the parameter dictionary\n",
    "                            params[key].append(line.split(': ')[-1])   \n",
    "                        \n",
    "                        # remove duplicates from each key's list of values\n",
    "                        params[key] = list(set(params[key]))\n",
    "\n",
    "            # Put parameters dictionary into the required Json format and add to kv dictionary for each run title.    \n",
    "            for key in params.keys():\n",
    "                parameterList.append(\n",
    "                    {\n",
    "                        \"parameter\": key,\n",
    "                        \"value\": params[key]\n",
    "                    }\n",
    "                )\n",
    "            sim_kv[title]['parameters'] = parameterList\n",
    "\n",
    "sim_kv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyList = ['Canopy', 'Route']\n",
    "# params = {}\n",
    "\n",
    "# for key in keyList:\n",
    "#         params[key] = []\n",
    "\n",
    "# for el in basinList:\n",
    "#     for line in el:\n",
    "#         for key in keyList:\n",
    "#             if f'{key}: ' in line:\n",
    "#                 # print (key, line.split(': ')[-1])\n",
    "#                 params[key].append(line.split(': ')[-1])   \n",
    "#                 # print (params[key])\n",
    "#             params[key] = list(set(params[key]))\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the .gage file\n",
    "with open(os.path.join(prj_dir,f'{prj_name}.gage'), 'r') as r:\n",
    "    gage_file = r.readlines()\n",
    "gage_file = [s.strip('\\n') for s in gage_file]\n",
    "# gage_file\n",
    "\n",
    "line_start = 0\n",
    "gageList = []\n",
    "for i,v in enumerate(gage_file):\n",
    "        if v == 'End:':\n",
    "                # If not the beginning of the file, skip a blank line (+1) for the start of the subList.\n",
    "                if len(gageList) > 0:\n",
    "                        gageList.append(gage_file[line_start+1:i])\n",
    "                else:\n",
    "                        gageList.append(gage_file[line_start:i])\n",
    "                line_start = i+1\n",
    "# gageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gage in .gage file, Get gage type and associated dss file name.\n",
    "sim_kv['Gage DSS Files'] ={}\n",
    "for gage in gageList:\n",
    "    # get gage title\n",
    "    title = gage[0].split(\":\")[1].strip()\n",
    "    # Init ditionary for each gage title\n",
    "    sim_kv['Gage DSS Files'][title] = {}\n",
    "    # findList is used to search the wanted .gage file fields for each gage title.\n",
    "    findList = [\"Gage Type\", \"DSS File Name\"]\n",
    "    # search each gage for the keys in findList, append as key:value pairs for each gage title.\n",
    "    for find_key in findList:\n",
    "        found_value = [s for s in gage if find_key in s]\n",
    "        \n",
    "        # Omit blank fields by testing the length\n",
    "        if len(found_value) > 0:\n",
    "            found_value = found_value[0].split(\":\")[1:][0].strip()\n",
    "            \n",
    "            sim_kv['Gage DSS Files'][title][find_key] = found_value\n",
    "\n",
    "    # Remove gage titles that did not contain the findList fields.\n",
    "    if len(sim_kv['Gage DSS Files'][title]) == 0:\n",
    "        del sim_kv['Gage DSS Files'][title]\n",
    "\n",
    "# sim_kv['Gage DSS Files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Flow', 'location': 'Amite_HMS_TimeSeries.dss'},\n",
       " {'title': 'Precipitation', 'location': 'rainfall_stageIV.dss'}]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = []\n",
    "# get values from dictionary in to a list without the keys.\n",
    "for key, value in sim_kv['Gage DSS Files'].items():\n",
    "    test_list.append(value)\n",
    "\n",
    "# Get a list of unique DSS File values in a list.\n",
    "gage_dss_files = []\n",
    "for t in test_list:\n",
    "    gage_dss_files.append(t['DSS File Name'])\n",
    "gage_dss_files = list(set(gage_dss_files))\n",
    "\n",
    "# Create list in the format needed for the hms simulation json.\n",
    "input_files = []\n",
    "for gage_dss_file in gage_dss_files:\n",
    "    for key, value in sim_kv['Gage DSS Files'].items():\n",
    "        # print (key, value)\n",
    "        if sim_kv['Gage DSS Files'][key]['DSS File Name'] == gage_dss_file:\n",
    "            \n",
    "            input_files.append(\n",
    "                {\n",
    "                    \"title\": sim_kv['Gage DSS Files'][key]['Gage Type'],\n",
    "                    \"location\": gage_dss_file\n",
    "                }\n",
    "            )\n",
    "# Remove duplicates from list\n",
    "input_files = [dict(t) for t in {tuple(d.items()) for d in input_files}]\n",
    "\n",
    "# append list to key value dictionary.\n",
    "sim_kv['input_files'] = input_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Basin': 'Consensus (Generic)',\n",
       " 'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "  {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "  {'parameter': 'Transform', 'value': ['Clark']},\n",
       "  {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "  {'parameter': 'Route', 'value': ['Muskingum Cunge']}],\n",
       " 'DSS File': 'August_2016.dss',\n",
       " 'Precip': 'Specified Hyetographs',\n",
       " 'Control': 'Aug2016',\n",
       " 'Description': 'Flood of August 2016',\n",
       " 'Start Date': '5 August 2016',\n",
       " 'End Date': '17 August 2016',\n",
       " 'Time Interval': '60'}"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_kv['August 2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'August 2016': {'Basin': 'Consensus (Generic)',\n",
       "  'DSS File': 'August_2016.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Aug2016',\n",
       "  'Description': 'Flood of August 2016',\n",
       "  'Start Date': '5 August 2016',\n",
       "  'End Date': '17 August 2016',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'Barry': {'Basin': 'Consensus (Generic)',\n",
       "  'DSS File': 'Barry.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Barry',\n",
       "  'Description': 'Hurricane Barry',\n",
       "  'Start Date': '2 July 2019',\n",
       "  'End Date': '25 July 2019',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'Gustav': {'Basin': 'Consensus (Generic)',\n",
       "  'DSS File': 'Gustav.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Gustav',\n",
       "  'Description': 'Hurricanes Gustav and Ike',\n",
       "  'Start Date': '21 August 2008',\n",
       "  'End Date': '19 September 2008',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'Isaac': {'Basin': 'Consensus (Generic)',\n",
       "  'DSS File': 'Isaac.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Isaac',\n",
       "  'Description': 'Hurricane Isaac',\n",
       "  'Start Date': '12 August 2012',\n",
       "  'End Date': '10 September 2012',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'Katrina': {'Basin': 'Consensus (Generic)',\n",
       "  'DSS File': 'Katrina.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'Katrina',\n",
       "  'Description': 'Hurricane Katrina',\n",
       "  'Start Date': '13 August 2005',\n",
       "  'End Date': '8 September 2005',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'March 2016': {'Basin': 'Consensus (March 2016)',\n",
       "  'DSS File': 'March_2016.dss',\n",
       "  'Precip': 'Specified Hyetographs',\n",
       "  'Control': 'March 2016',\n",
       "  'Description': 'March 2016',\n",
       "  'Start Date': '1 March 2016',\n",
       "  'End Date': '30 March 2016',\n",
       "  'Time Interval': '60',\n",
       "  'parameters': [{'parameter': 'Canopy', 'value': ['Simple']},\n",
       "   {'parameter': 'LossRate', 'value': ['Deficit Constant']},\n",
       "   {'parameter': 'Transform', 'value': ['Clark']},\n",
       "   {'parameter': 'Baseflow', 'value': ['Monthly Constant']},\n",
       "   {'parameter': 'Route', 'value': ['Muskingum Cunge']}]},\n",
       " 'Gage DSS Files': {'Amite': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'AmiteBaywood': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'AmiteDenhamSprings': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'BalaChittoCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'BayouManchac': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'BeaverCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'BigCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'BlindRiver': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Chappepeela': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'ClayCutJones': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'ColyelCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'ComiteCentral': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'DarlingCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'EastFork': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'GraysCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Henderson': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'LittleTangipahoa': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Maurepas': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'MillsCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Natalbany': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'RedwoodCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'SandyCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Tangipahoa': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'TerrysCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Tickfaw': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'TickfawHolden': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'TwelvemileCreek': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'UpperAmite': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'UpperComite': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'UpperNatalbany': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'UpperTickfaw': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'WestFork': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'WhiteBayou': {'Gage Type': 'Precipitation',\n",
       "   'DSS File Name': 'rainfall_stageIV.dss'},\n",
       "  'Amite at Darlington': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Amite at Denham Springs': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Amite at Magnolia': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Comite At Comite': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Comite At Olive Branch': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Natalbany at Baptist': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Tangipahoa at Osyka': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Tangipahoa at Robert': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Tickfall at Holden': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Tickfall at Liverpool': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'},\n",
       "  'Tickfall at Montpelier': {'Gage Type': 'Flow',\n",
       "   'DSS File Name': 'Amite_HMS_TimeSeries.dss'}}}"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# open the simulation Json template, del unnecessary keys, update, add, export \n",
    "with open(r\"C:\\py\\hec_meta_extract\\example\\input\\json\\hms_simulation.json\", 'r') as f:\n",
    "            simulation_template_json = json.load(f)\n",
    "\n",
    "# keys to drop from json template\n",
    "drop_keys = ['_id', 'model_application', 'model_software', 'linked_resources','type']\n",
    "for key in drop_keys:\n",
    "    del model_template_json[key]\n",
    "\n",
    "simulation_template_json['description'] = kv['Project']['Description']\n",
    "simulation_template_json['title'] = f\"{kv['Project']['Title']} HEC-HMS Model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-07-02'"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startDate = datetime.strptime(sim_kv['Barry']['Start Date'], '%d  %B %Y').strftime('%Y-%m-%d')\n",
    "startDate "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hec_meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76ecb9a863def9b21f44032a5e87e3ce48447e177fd90c9ddac9401b285a34c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
